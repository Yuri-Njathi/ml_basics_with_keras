{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go save those models already.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model progress can be saved during and after training.\n",
    "\n",
    "A model can resume where it left off and avoid long training times.\n",
    "\n",
    "Saving means you can share your model and others can recreate your work.\n",
    "\n",
    "When publishing research models and techniques, most ml practitioners share:\n",
    "    -code to create the model.\n",
    "    -the trained weights, or parameters.\n",
    "    \n",
    "    \n",
    "    Sharing this data helps others understand how the model works and try it themselves with new data.\n",
    "    \n",
    "    There are different ways to save Tensorflow models - depending on the API used.\n",
    "'''\n",
    "print(\"Go save those models already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an exapmle dataset.\n",
    "'''\n",
    "Demonstrate how to save and load weights.\n",
    "Using MNIST dataset.\n",
    "'''\n",
    "(train_images,train_labels),(test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1,28 * 28)/255.0\n",
    "test_images = test_images[:1000].reshape(-1,28 * 28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DEFINE THE MODEL.\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu',input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "#create basic model instance.\n",
    "model = create_model()\n",
    "\n",
    "#Display structure of model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 1.1578 - accuracy: 0.6781\n",
      "Epoch 00001: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1428 - accuracy: 0.6810 - val_loss: 0.7502 - val_accuracy: 0.7680\n",
      "Epoch 2/10\n",
      " 896/1000 [=========================>....] - ETA: 0s - loss: 0.4243 - accuracy: 0.8862\n",
      "Epoch 00002: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 958us/sample - loss: 0.4298 - accuracy: 0.8840 - val_loss: 0.5447 - val_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.2932 - accuracy: 0.9292\n",
      "Epoch 00003: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 907us/sample - loss: 0.2876 - accuracy: 0.9320 - val_loss: 0.5104 - val_accuracy: 0.8430\n",
      "Epoch 4/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.2168 - accuracy: 0.9450\n",
      "Epoch 00004: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.2173 - accuracy: 0.9440 - val_loss: 0.5037 - val_accuracy: 0.8490\n",
      "Epoch 5/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.1644 - accuracy: 0.9580\n",
      "Epoch 00005: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1608 - accuracy: 0.9590 - val_loss: 0.4381 - val_accuracy: 0.8530\n",
      "Epoch 6/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.1161 - accuracy: 0.9771\n",
      "Epoch 00006: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.1157 - accuracy: 0.9770 - val_loss: 0.4472 - val_accuracy: 0.8550\n",
      "Epoch 7/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9839\n",
      "Epoch 00007: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0905 - accuracy: 0.9840 - val_loss: 0.3884 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.0680 - accuracy: 0.9937\n",
      "Epoch 00008: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0672 - accuracy: 0.9940 - val_loss: 0.4130 - val_accuracy: 0.8700\n",
      "Epoch 9/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 0.9960\n",
      "Epoch 00009: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 967us/sample - loss: 0.0488 - accuracy: 0.9960 - val_loss: 0.3999 - val_accuracy: 0.8740\n",
      "Epoch 10/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.0385 - accuracy: 0.9989\n",
      "Epoch 00010: saving model to model/saveandload.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0390 - accuracy: 0.9990 - val_loss: 0.4229 - val_accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cbb222668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Checkpoints during training.\n",
    "'''\n",
    "tf.keras.callbacks.ModelCheckpoint()\n",
    "'''\n",
    "\n",
    "# Model itaekwa wapi?\n",
    "checkpoint_path = \"model/saveandload.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#Model itaitwa aje?\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=True)\n",
    "\n",
    "#Train model.\n",
    "model.fit(train_images,\n",
    "          train_labels,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images,test_labels),\n",
    "          callbacks = [cp_callback] #Model inasaviwa after kila epoch.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Basic classification Classify images of clothing.ipynb'\r\n",
      " \u001b[0m\u001b[01;34mmodel\u001b[0m/\r\n",
      " README.md\r\n",
      "'Save and Load Models.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "ls #the checkpoint iko kwa folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 2.3934 - accuracy: 0.1160\n",
      "Untrained model accuracy : 11.60%\n"
     ]
    }
   ],
   "source": [
    "# to create a new, untrained model of same architecture.\n",
    "\n",
    "#create fresh model.\n",
    "model = create_model()\n",
    "\n",
    "#Evaluate model.\n",
    "loss, acc = model.evaluate(test_images,test_labels,verbose=2)\n",
    "print(\"Untrained model accuracy : {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.4974 - accuracy: 0.8610\n",
      "Restored model, accuracy: 86.10%\n"
     ]
    }
   ],
   "source": [
    "# load the weights.\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model.\n",
    "loss,acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "\n",
      "Epoch 00005: saving model to model/training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to model/training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to model/training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to model/training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to model/training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to model/training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to model/training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to model/training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to model/training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to model/training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c741aa908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint callback options.\n",
    "'''\n",
    "The callback provides several options to provide unique names for checkpoints \n",
    "and adjust the checkpointing frequency.\n",
    "'''\n",
    "\n",
    "# Include the epoch in the file name using `str.format`\n",
    "checkpoint_path = \"model/training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs.\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5\n",
    ")\n",
    "\n",
    "#Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format.\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# Train the model with the new callback.\n",
    "model.fit(train_images,\n",
    "          train_labels,\n",
    "          epochs=50,\n",
    "          callbacks=[cp_callback],\n",
    "          validation_data=(test_images,test_labels),\n",
    "          verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                        cp-0025.ckpt.index\r\n",
      "cp-0000.ckpt.data-00000-of-00001  cp-0030.ckpt.data-00000-of-00001\r\n",
      "cp-0000.ckpt.index                cp-0030.ckpt.index\r\n",
      "cp-0005.ckpt.data-00000-of-00001  cp-0035.ckpt.data-00000-of-00001\r\n",
      "cp-0005.ckpt.index                cp-0035.ckpt.index\r\n",
      "cp-0010.ckpt.data-00000-of-00001  cp-0040.ckpt.data-00000-of-00001\r\n",
      "cp-0010.ckpt.index                cp-0040.ckpt.index\r\n",
      "cp-0015.ckpt.data-00000-of-00001  cp-0045.ckpt.data-00000-of-00001\r\n",
      "cp-0015.ckpt.index                cp-0045.ckpt.index\r\n",
      "cp-0020.ckpt.data-00000-of-00001  cp-0050.ckpt.data-00000-of-00001\r\n",
      "cp-0020.ckpt.index                cp-0050.ckpt.index\r\n",
      "cp-0025.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "ls {\"model/training_2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/training_2/cp-0050.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest= tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.6898 - accuracy: 0.8750\n",
      "Restored model, accuracy: 87.50\n"
     ]
    }
   ],
   "source": [
    "#Test and reset model.\n",
    "\n",
    "# Create a new model instance.\n",
    "model = create_model()\n",
    "\n",
    "#Load the previously saved weights.\n",
    "model.load_weights(latest)\n",
    "\n",
    "#Re-evaluate the model.\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1 - 0s - loss: 0.4718 - accuracy: 0.8610\n",
      "Restored model, accuracy: 86.10%\n"
     ]
    }
   ],
   "source": [
    "# Manually save weights to .ckpt\n",
    "# lAter eXplorE HDF5 with a .h5 extension. https://www.tensorflow.org/guide/keras/save_and_serialize#weights_only_saving_in_savedmodel_format\n",
    "\n",
    "# Okoa hizo weights.\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "#Tengeneza a new model instance.\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Rudisha hizo weights.\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# Evaluate hiyo model.\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1290 - accuracy: 0.6830\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1s 517us/sample - loss: 0.4279 - accuracy: 0.8770\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 482us/sample - loss: 0.2947 - accuracy: 0.9200\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 495us/sample - loss: 0.2011 - accuracy: 0.9580\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1s 530us/sample - loss: 0.1499 - accuracy: 0.9660\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:From /home/caesar/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "If we export these checkpoints vile ziko, tutahitaji kuexport model's \n",
    "architecture and training configuration.\n",
    "\n",
    "The entire model can be saved into two different file formats (SavedModel and HDF5).\n",
    "\n",
    "Saving a fully-saved model is very useful and one can load them using Tensorflow.js (Saved Model, HDF5)\n",
    "and train them and run them in web browsers or convert them to run on mobile devices using Tensorflow Lite. (Saved Model,HDF5)\n",
    "\n",
    "'''\n",
    "# Create and train a new model instance.\n",
    "model = create_model()\n",
    "model.fit(train_images,train_labels,epochs=5)\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34massets\u001b[0m/  saved_model.pb  \u001b[01;34mvariables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls {'saved_model/my_model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.1361 - accuracy: 0.6740\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1s 565us/sample - loss: 0.4042 - accuracy: 0.8890\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1s 567us/sample - loss: 0.2764 - accuracy: 0.9280\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1s 532us/sample - loss: 0.1993 - accuracy: 0.9520\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1s 609us/sample - loss: 0.1450 - accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "# Save entire model as a HDF5\n",
    "\n",
    "'saved_model/my_model'\n",
    "\n",
    "#Create and train a new model instance.\n",
    "model = create_model()\n",
    "model.fit(train_images,train_labels, epochs=5)\n",
    "\n",
    "\n",
    "#Save the entire model to a HDF5 file.\n",
    "# The .h5 extension indicates that the model should be saved to HDF5.\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Basic classification Classify images of clothing.ipynb'\r\n",
      " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\r\n",
      " \u001b[01;34mmodel\u001b[0m/\r\n",
      " my_model.h5\r\n",
      " README.md\r\n",
      "'Save and Load Models.ipynb'\r\n",
      " \u001b[01;34msaved_model\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1 - 0s - loss: 0.6321 - accuracy: 0.8560\n",
      "Restored model, accuracy: 85.60%\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model from that file.\n",
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Show the model architecture.\n",
    "new_model.summary()\n",
    "\n",
    "\n",
    "#Check its accuracy.\n",
    "loss, acc = new_model.evaluate(test_images,test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.1455 - accuracy: 0.9600 - val_loss: 0.4957 - val_accuracy: 0.8360\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 825us/sample - loss: 0.0922 - accuracy: 0.9820 - val_loss: 0.4563 - val_accuracy: 0.8530\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 779us/sample - loss: 0.0665 - accuracy: 0.9930 - val_loss: 0.4160 - val_accuracy: 0.8640\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 918us/sample - loss: 0.0509 - accuracy: 0.9970 - val_loss: 0.3964 - val_accuracy: 0.8710\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 856us/sample - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.3953 - val_accuracy: 0.8730\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 868us/sample - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.4203 - val_accuracy: 0.8720\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 999us/sample - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.8660\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.3994 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 771us/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.8700\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 891us/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "# Kuendelea training hiyo model.\n",
    "history = new_model.fit(train_images,train_labels,epochs=10,validation_data=(test_images,test_labels),\n",
    "                       shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save custom objects.\n",
    "\n",
    "'''\n",
    "The difference between HDF5 and SavedModel is that HDF5 uses on=bject configs to save the model\n",
    "architecture, while SvaedModel saves the execution graph.\n",
    "\n",
    "SavedModels are able to save custom objects like sub-classed models and custom layers without requiring the original code.\n",
    "To save custom objects to HDF5, you must do the following:\n",
    "\n",
    "Define a get_config method in your object, and optionally a from_config classmethod.\n",
    "get_config(self) returns a JSON-serializable dictionary of parameters needed to recreate the object.\n",
    "from_config(cls, config) uses the returned config from get_config to create a new object. By default, this function will use the config as initialization kwargs (return cls(**config)).\n",
    "Pass the object to the custom_objects argument when loading the model. The argument must be a dictionary mapping the string class name to the Python class. E.g. tf.keras.models.load_model(path, custom_objects={'CustomLayer': CustomLayer})\n",
    "See the Writing layers and models from scratch tutorial for examples of custom objects and get_config.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
